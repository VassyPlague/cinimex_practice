{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f494b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_metric\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a817b",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2719c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/intern-lab/.cache/huggingface/datasets/csv/default-6ceff85eb6c91486/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af21c79aed1b49ab892cac4384198d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\intern-lab\\.cache\\huggingface\\datasets\\csv\\default-6ceff85eb6c91486\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-4e7348e6d444e196.arrow\n",
      "Found cached dataset csv (C:/Users/intern-lab/.cache/huggingface/datasets/csv/default-878044040cc39661/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bb5f22c49f49a78aab5f66a7671e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\intern-lab\\.cache\\huggingface\\datasets\\csv\\default-878044040cc39661\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-be471511e3649998.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'words', 'ner', 'task'],\n",
       "        num_rows: 157\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÐºÐ°ÐºÐ°Ñ-Ñ‚Ð¾ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² ner ÑÐ¿Ð¸ÑÐ¾Ðº Ð½Ð° ÑÐ°Ð¼Ð¾Ð¼ Ð´ÐµÐ»Ðµ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ string\n",
    "tr = load_dataset('csv', data_files='data/ner/trainfold_0.csv').remove_columns('Unnamed: 0')\n",
    "tr = tr.shuffle(seed= 14)\n",
    "ts = load_dataset('csv', data_files='data/ner/testfold_0.csv').remove_columns('Unnamed: 0')\n",
    "ts = ts.shuffle(seed= 14)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f0bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð² Ðº Ñ‚ÐµÐ³Ñƒ\n",
    "id2label = {0 : 'O', 1 : 'PAT', 2 : 'PHY', 3 : 'PAT-START', 4 : 'PHY-START'}\n",
    "label2id = {'O' : 0, 'PAT' : 1, 'PHY' : 2, 'PAT-START' : 3, 'PHY-START' : 4}\n",
    "label_list = ['O', 'PAT', 'PHY', 'PAT-START', 'PHY-START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a64eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð· str Ð² list\n",
    "def from_str_to_list(data):\n",
    "    out = []\n",
    "    for i in data:\n",
    "        out.append(ast.literal_eval(i))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6023143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² list\n",
    "train = tr['train']\n",
    "test = ts['train']\n",
    "\n",
    "trner = train['ner']\n",
    "trwords = train['words']\n",
    "\n",
    "tsner = test['ner']\n",
    "tswords = test['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066a1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\intern-lab\\.cache\\huggingface\\datasets\\csv\\default-6ceff85eb6c91486\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-9842681c3e77d2a5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\intern-lab\\.cache\\huggingface\\datasets\\csv\\default-878044040cc39661\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-687d16dc0cf5f9a5.arrow\n"
     ]
    }
   ],
   "source": [
    "# Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² list\n",
    "train = train.remove_columns('ner')\n",
    "train = train.add_column('ner',from_str_to_list(trner))\n",
    "train = train.remove_columns('words')\n",
    "train = train.add_column('words',from_str_to_list(trwords))\n",
    "\n",
    "test = test.remove_columns('ner')\n",
    "test = test.add_column('ner',from_str_to_list(tsner))\n",
    "test = test.remove_columns('words')\n",
    "test = test.add_column('words',from_str_to_list(tswords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb4d2c",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc54e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸\n",
    "max_length = 512\n",
    "def tokenize_and_align_labels(examples): # trunc texts and labels\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"words\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c5210e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenized_train = train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9985f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207b1352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intern-lab\\AppData\\Local\\Temp\\ipykernel_11152\\339056260.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "# Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46f7fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3,\n",
    "                                                        label2id=label2id,\n",
    "                                                        id2label = id2label,\n",
    "                                                        ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de551768",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407c203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
    "task = \"ner\" \n",
    "args = TrainingArguments(\n",
    "    output_dir = 'models',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 1e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = tokenized_train,\n",
    "    eval_dataset = tokenized_test,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71128b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: words, task, id, ner. If words, task, id, ner are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\intern-lab\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 628\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 628\n",
      "  Number of trainable parameters = 177265155\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# ÑÑ‚Ð°Ñ€Ñ‚ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¸ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55d8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
